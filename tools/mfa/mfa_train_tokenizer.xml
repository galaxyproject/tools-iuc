<tool id="mfa_train_tokenizer" name="MFA Train Tokenizer" version="@TOOL_VERSION@+@VERSION_SUFFIX@" profile="@PROFILE@">
    <description>train a tokenizer model from a corpus and dictionary</description>
    <macros>
        <import>macros.xml</import>
    </macros>
    <expand macro="requirements" />
    <command detect_errors="exit_code"><![CDATA[
        @PREPARE_ENV@

        ## 1. Prepare Corpus
        mkdir -p corpus_dir &&
        #if $corpus_source.ext == 'zip':
            unzip -q '$corpus_source' -d corpus_dir &&
        #else:
            tar -xf '$corpus_source' -C corpus_dir &&
        #end if

        ## 2. Prepare Dictionary
        #if $dictionary_type.source == 'history':
            ln -s '$dictionary_type.dictionary_file' input_dictionary.txt &&
            #set $dict_arg = 'input_dictionary.txt'
        #else:
            #set $dict_arg = $dictionary_type.builtin_dictionary.fields.path
        #end if

        ## 3. Prepare Config (Optional)
        #if $config_file:
            ln -s '$config_file' config.yaml &&
        #end if

        ## 4. Run Training
        ## Syntax: mfa train_tokenizer [OPTIONS] SOURCE_DIRECTORY DICTIONARY_PATH OUTPUT_MODEL_PATH
        mfa train_tokenizer
            corpus_dir
            $dict_arg
            '$output_model'
            --job_name 'mfa_train_tok_job'
            #if $config_file:
                --config_path config.yaml
            #end if
            #if $phonetisaurus:
                --phonetisaurus
            #end if
            --order $order

    ]]></command>

    <inputs>
        <param name="corpus_source" type="data" format="zip,tar,tar.gz" label="Corpus Archive" help="Archive containing text files used for training." />

        <conditional name="dictionary_type">
            <param name="source" type="select" label="Input Dictionary" help="The dictionary defines the valid words that the tokenizer should learn to identify.">
                <option value="builtin" selected="true">Use a built-in MFA Dictionary</option>
                <option value="history">Upload from History</option>
            </param>
            <when value="builtin">
                <param name="builtin_dictionary" type="select" label="Select Dictionary">
                    <options from_data_table="mfa_dictionary">
                        <column name="value" index="0"/>
                        <column name="dbkey" index="1"/>
                        <column name="name" index="2"/>
                        <column name="path" index="3"/>
                    </options>
                </param>
            </when>
            <when value="history">
                <param name="dictionary_file" type="data" format="txt" label="Dictionary File" />
            </when>
        </conditional>

        <param name="phonetisaurus" type="boolean" truevalue="--phonetisaurus" falsevalue="" checked="false" label="Use Phonetisaurus" help="If checked, use Phonetisaurus for training (often faster/better for G2P-style tasks) instead of Pynini." />
        
        <param name="order" type="integer" value="5" min="1" max="10" label="N-gram Order" help="The order of the model. Higher numbers capture more context." />

        <param name="config_file" type="data" format="yaml" optional="true" label="Configuration File" help="Optional configuration for advanced training parameters." />

    </inputs>

    <outputs>
        <data name="output_model" format="zip" label="${tool.name} on ${on_string}: Trained Tokenizer" />
    </outputs>

    <tests>
        <test expect_num_outputs="1">
            <param name="corpus_source" value="test_corpus.zip" ftype="zip" />
            
            <conditional name="dictionary_type">
                <param name="source" value="history" />
                <param name="dictionary_file" value="test_dict.txt" />
            </conditional>
            
            <param name="order" value="2" />
            <output name="output_model" file="tokenizer_model.zip" compare="sim_size" delta="10000" />
        </test>
    </tests>

    <help><![CDATA[
**Montreal Forced Aligner: Train Tokenizer**

This tool trains a model to split continuous text into words.

**When to use this?**

This is primarily for languages like Japanese, Chinese, or Thai that do not use whitespace to separate words. 

* You provide a **Dictionary** (the list of valid words).
* You provide a **Corpus** (text data).
* MFA learns the statistical probability of character sequences to determine where word boundaries should be placed.

**Output**

A `.zip` file containing the Tokenizer Model. You can use this in the **MFA Tokenize** tool.

    ]]></help>
    <expand macro="creators" />
    <expand macro="citations" />
</tool>